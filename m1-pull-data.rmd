# Set Constants
```{r purl = FALSE}
DIR = Sys.getenv('EF_DIR')
DL_DIR = file.path(Sys.getenv('EF_DIR'), 'tmp')
INPUT_DIR = file.path(Sys.getenv('EF_DIR'), 'model-inputs') # Path to directory with constants.r (SQL DB info, SFTP info, etc.)
OUTPUT_DIR = file.path(Sys.getenv('EF_DIR'), 'model-outputs')
VINTAGE_DATE_START = as.Date('2021-10-20')
VINTAGE_DATE_END = Sys.Date()
```


# Initialize
```{r}
library(tidyverse)
library(data.table)
library(jsonlite)
library(lubridate)
library(httr)
library(rvest)
library(DBI)
library(econforecasting)

# Set working directory
setwd(DIR)

# Create temp directory if doesn't exist
if (dir.exists(DL_DIR)) unlink(DL_DIR, recursive = TRUE)
dir.create(DL_DIR, recursive = TRUE)

# Read constants
source(file.path(INPUT_DIR, 'constants.r'))

# Set top level variables - params, history, model
p = list(
	variablesDf = readxl::read_excel(file.path(INPUT_DIR, 'inputs.xlsx'), sheet = 'all-variables'),
	forecastsDf = readxl::read_excel(file.path(INPUT_DIR, 'inputs.xlsx'), sheet = 'ext-forecasts'),
	releasesDf = readxl::read_excel(file.path(INPUT_DIR, 'inputs.xlsx'), sheet = 'releases'),
	VINTAGE_DATE_START = VINTAGE_DATE_START,
	VINTAGE_DATE_END = VINTAGE_DATE_END,
	VINTAGE_DATES = seq(VINTAGE_DATE_START, VINTAGE_DATE_END, by = '1 day')
	) %>%
	c(., list(variables = purrr::transpose(.$variablesDf, .names = .$variablesDf$varname)))

h = list()
m = list()


# Check that each variable has a correct release
dplyr::left_join(p$variablesDf, p$releasesDf, by = 'relkey') %>%
	.$relname %>%
	keep(., ~ is.na(.)) %>%
	{if(length(.) != 0) stop('Error')}
```


# Get Historical Data

## FRED Releases
```{r}
local({
	
	releaseDf =
		p$variablesDf %>%
		dplyr::group_by(., relkey) %>%
		dplyr::summarize(., n_varnames = n(), varnames = jsonlite::toJSON(fullname), .groups = 'drop') %>%
		dplyr::left_join(
			.,
			p$variablesDf %>%
				dplyr::filter(., nc_dfm_input == 1) %>%
				dplyr::group_by(., relkey) %>%
				dplyr::summarize(., n_dfm_varnames = n(), dfm_varnames = jsonlite::toJSON(fullname), .groups = 'drop'),
			p$variablesDf %>%
				dplyr::group_by(., relkey) %>%
				dplyr::summarize(., n_varnames = n(), varnames = jsonlite::toJSON(fullname), .groups = 'drop'),
			by = 'relkey'
		) %>%
		dplyr::left_join(
			.,
			p$releasesDf,
			by = 'relkey'
		) %>%
		# Now create a column of included releaseDates
		dplyr::left_join(
			.,
			purrr::map_dfr(purrr::transpose(dplyr::filter(., relsc == 'fred')), function(x)
				httr::RETRY(
					'GET', 
					str_glue(
						'https://api.stlouisfed.org/fred/release/dates?',
						'release_id={x$relsckey}&realtime_start=2020-01-01',
						'&include_release_dates_with_no_data=true&api_key={CONST$FRED_API_KEY}&file_type=json'
						 ),
					times = 10
					) %>%
			        httr::content(., as = 'parsed') %>%
					.$release_dates %>%
					sapply(., function(y) y$date) %>%
					tibble(relkey = x$relkey, reldates = .)
				) %>%
				dplyr::group_by(., relkey) %>%
				dplyr::summarize(., reldates = jsonlite::toJSON(reldates), .groups = 'drop')
				,
			by = 'relkey'
		)
	
	p$releasesDf <<- releaseDf
})
```


## FRED
```{r}
local({
	
	fredRes =
		p$variables %>%
		purrr::keep(., ~ .$source == 'fred') %>%
		lapply(., function(x) {
			
			message('Getting data ... ', x$varname)

			# Get series data
			dataDf =
				getDataFred(x$sckey, CONST$FRED_API_KEY, .freq = x$freq, .returnVintages = TRUE, .vintageDate = as_date(with_tz(now(), tz = 'America/Chicago')), .verbose = F) %>%
			  	dplyr::filter(., vintageDate <= VINTAGE_DATE) %>%
			  	dplyr::filter(., vintageDate == max(vintageDate)) %>%
			  	dplyr::select(., -vintageDate) %>%
				dplyr::transmute(., date = obsDate, value) %>%
				dplyr::filter(., date >= as.Date('2010-01-01'))

			list(dataDf = dataDf)
			})
	
	
	for (varname in names(fredRes)) {
		p$variables[[varname]]$rawData <<- fredRes[[varname]]$dataDf
	}
})
```

## Yahoo Finance
```{r}
local({
	
	res =
		p$variables %>%
    	purrr::keep(., ~ .$source == 'yahoo') %>%
		lapply(., function(x) {
			url =
				paste0(
					'https://query1.finance.yahoo.com/v7/finance/download/', x$sckey,
					'?period1=', '1262304000', # 12/30/1999
					'&period2=', as.numeric(as.POSIXct(Sys.Date() + lubridate::days(1))),
					'&interval=1d',
					'&events=history&includeAdjustedClose=true'
				)
			data.table::fread(url, showProgress = FALSE) %>%
				.[, c('Date', 'Adj Close')]	%>%
				setnames(., new = c('date', 'value')) %>%
				as_tibble(.) %>%
				dplyr::filter(., value != 'null') %>% # Bug with yahoo finance returning null for date 7/22/21 as of 7/23
				dplyr::mutate(., value = as.numeric(value)) %>% #
				dplyr::filter(., date <= p$VINTAGE_DATE) %>%
				return(.)
		})
	
	for (varname in names(res)) {
		p$variables[[varname]]$rawData <<- res[[varname]]
	}
})
```



# Aggregate Frequencies

## Move to Hist Object
```{r}
local({
	
	 res =
	 	p$variables %>%
	 	purrr::keep(., ~ is_tibble(.$rawData)) %>%
	 	lapply(., function(x)
	 		list(x$rawData) %>%
	 			setNames(., x$freq)
	 		)
	 
	for (varname in names(res)) {
		p$variables[[varname]]$h$base <<- res[[varname]]
	}
})
```

## Monthly Agg
```{r}
local({
	
    res =
        # Get all daily/weekly varnames with pre-existing data
        p$variables %>%
        purrr::keep(., ~ .$freq %in% c('d', 'w') && is_tibble(.$rawData)) %>%
        # Add monthly values for current month
        lapply(., function(x) {
        	x$rawData %>%
        		dplyr::mutate(., date = as.Date(paste0(year(date), '-', month(date), '-01'))) %>%
        		dplyr::group_by(., date) %>%
        		dplyr::summarize(., value = mean(value), .groups = 'drop') %>%
        		dplyr::mutate(., freq = 'm') %>%
        		{
        			# Keep last-month aggregation despite possible missing data if set in inputs.xlsx
        			if (x$append_eom_with_currentval == 1) .
        			else dplyr::filter(., date != max(date))
        		}
        	})
    
	for (varname in names(res)) {
		p$variables[[varname]]$h$base$m <<- res[[varname]]
	}
})
```

## Quarterly Aggregation
```{r}
local({
	
    res =
        p$variables %>%
        purrr::keep(., ~ .$freq %in% c('d', 'w', 'm') && is_tibble(.$rawData)) %>%
        lapply(., function(x) {
        	message(x$varname)
        	x$h$base$m %>%
        		dplyr::mutate(., date = strdateToDate(paste0(year(date), 'Q', quarter(date)))) %>%
        		dplyr::group_by(., date) %>%
        		dplyr::summarize(., value = mean(value), .groups = 'drop', n = n()) %>%
        		# Only keep if all 3 monthly data exists (or imputed by previous chunk)
        		dplyr::filter(., n == 3) %>%
        		dplyr::select(., -n)
        	})
    
	for (varname in names(res)) {
		p$variables[[varname]]$h$base$q <<- res[[varname]]
	}
    
})
```



# Add Calculated Variables

## Trailing Inflation
```{r}
local({
	
    mDf =
    	p$variables$cpi$h$base$m %>%
    	dplyr::mutate(., value = (value/dplyr::lag(value, 13) - 1) * 100) %>%
    	na.omit(.)
    
    qDf =
    	mDf %>%
    	dplyr::mutate(., date = strdateToDate(paste0(year(date), 'Q', quarter(date)))) %>%
        dplyr::group_by(., date) %>%
        dplyr::summarize(., value = mean(value), .groups = 'drop', n = n()) %>%
        # Only keep if all 3 monthly data exists (or imputed by previous chunk)
        dplyr::filter(., n == 3) %>%
        dplyr::select(., -n)
    
    p$variables$inf$h$base$m <<- mDf
    p$variables$inf$h$base$q <<- qDf
})
```


## DNS Model - Interest Rates
Let tyield = ffr + dns_curve(ttm)
Exogenously choose ffr and tyield_10y_3m (negative of 10year - 3month spread; 3-month driven heavily by ffr)
```{r}
local({
  
    # Create tibble mapping tyield_3m to 3, tyield_1y to 12, etc.
    yieldCurveNamesMap =
        p$variables %>% 
        map_chr(., ~.$varname) %>%
        unique(.) %>%
        purrr::keep(., ~ str_sub(., 1, 1) == 't' & str_length(.) == 4) %>%
        tibble(varname = .) %>%
        dplyr::mutate(., ttm = as.numeric(str_sub(varname, 2, 3)) * ifelse(str_sub(varname, 4, 4) == 'y', 12, 1))
  
    # Create training dataset from SPREAD from ffr - fitted on last 3 months
    trainDf =
        purrr::map_dfr(yieldCurveNamesMap$varname, function(x) p$variables[[x]]$h$base$m %>% dplyr::mutate(., varname = x)) %>%
        dplyr::select(., -freq) %>%
        dplyr::filter(., date >= add_with_rollback(p$VINTAGE_DATE, months(-3))) %>%
        dplyr::right_join(., yieldCurveNamesMap, by = 'varname') %>%
        dplyr::left_join(., dplyr::transmute(p$variables$ffr$h$base$m, date, ffr = value), by = 'date') %>%
        dplyr::mutate(., value = value - ffr) %>%
        dplyr::select(., -ffr)
  
    # @param df: (tibble) A tibble continuing columns obsDate, value, and ttm
    # @param returnAll: (boolean) FALSE by default.
    # If FALSE, will return only the MAPE (useful for optimization).
    # Otherwise, will return a tibble containing fitted values, residuals, and the beta coefficients.
    getDnsFit = function(df, lambda, returnAll = FALSE) {
        df %>%
        dplyr::mutate(
        .,
        f1 = 1,
        f2 = (1 - exp(-1 * lambda * ttm))/(lambda * ttm),
        f3 = f2 - exp(-1 * lambda * ttm)
        ) %>%
        dplyr::group_by(date) %>%
        dplyr::group_split(.) %>%
        lapply(., function(x) {
        reg = lm(value ~ f1 + f2 + f3 - 1, data = x)
        dplyr::bind_cols(x, fitted = fitted(reg)) %>%
        dplyr::mutate(., b1 = coef(reg)[['f1']], b2 = coef(reg)[['f2']], b3 = coef(reg)[['f3']]) %>%
        dplyr::mutate(., resid = value - fitted)
        }) %>%
        dplyr::bind_rows(.) %>%
        {
        if (returnAll == FALSE) dplyr::summarise(., mse = mean(abs(resid))) %>% .$mse
        else .
        } %>%
        return(.)
    }

    # Find MSE-minimizing lambda value
    optimLambda =
        optimize(
            getDnsFit,
            df = trainDf,
            returnAll = FALSE,
            interval = c(-1, 1),
            maximum = FALSE
            )$minimum
  
    mDf =
        purrr::map_dfr(yieldCurveNamesMap$varname, function(x)
        		p$variables[[x]]$h$base$m %>% dplyr::mutate(., varname = x)
        	) %>%
        dplyr::select(., -freq) %>%
        dplyr::right_join(., yieldCurveNamesMap, by = 'varname') %>%
        dplyr::left_join(., dplyr::transmute(p$variables$ffr$h$base$m, date, ffr = value), by = 'date') %>%
        dplyr::mutate(., value = value - ffr) %>%
        dplyr::select(., -ffr) %>%
        getDnsFit(., lambda = optimLambda, returnAll = TRUE) %>%
        dplyr::group_by(., date) %>%
        dplyr::summarize(., tdns1 = unique(b1), tdns2 = unique(b2), tdns3 = unique(b3)) %>%
        tidyr::pivot_longer(., -date, names_to = 'varname')

    mDfs = mDf %>% split(., as.factor(.$varname)) %>% lapply(., function(x) x %>% dplyr::select(., -varname))

    qDfs = mDfs %>% lapply(., function(x) monthlyDfToQuarterlyDf(x))
  
    # Store DNS coefficients
    dnsCoefs =
        getDnsFit(df = trainDf, optimLambda, returnAll = TRUE) %>%
        dplyr::filter(., date == max(date)) %>%
        dplyr::select(., b1, b2, b3) %>%
        head(., 1) %>%
        as.list(.)

    dnsFitChart =
        getDnsFit(df = trainDf, optimLambda, returnAll = TRUE) %>%
        dplyr::filter(., date == max(date)) %>%
        dplyr::arrange(., ttm) %>%
        ggplot(.) +
        geom_point(aes(x = ttm, y = value)) +
        geom_line(aes(x = ttm, y = fitted))


    for (varname in names(mDfs)) {
        p$variables[[varname]]$h$base$m <<- mDfs[[varname]]
        p$variables[[varname]]$h$base$q <<- qDfs[[varname]]
    }

    dnsFitChart
    
    m$dnsLambda <<- optimLambda
    m$dnsCoefs <<- dnsCoefs
    m$dnsFitChart <<- dnsFitChart
    m$dnsYieldCurveNamesMap <<- yieldCurveNamesMap
})
```

## Other Interest Rate Spreads
```{r}
local({
    
    resDfs =
        tribble(
            ~varname, ~var1, ~var2,
            'mort30yt30yspread', 'mort30y', 't30y',
            'mort15yt10yspread', 'mort15y', 't10y'
            ) %>%
        purrr::transpose(., .names = .$varname) %>%
        lapply(., function(x) {
            m =
                dplyr::inner_join(
                    p$variables[[x$var1]]$h$base$m,
                    p$variables[[x$var2]]$h$base$m %>% dplyr::rename(., v2 = value),
                    by = 'date'
                ) %>%
                dplyr::transmute(., date, value = value - v2)
            q = monthlyDfToQuarterlyDf(m)
            list(m = m, q = q)
            })
    
  for (varname in names(resDfs)) {
	p$variables[[varname]]$h$base <<- resDfs[[varname]]
  }
})
```



# Transformations

## Deseasonalize
```{r}
local({
	# seasDf =
	# 	p$h$sourceDf %>%
	# 	dplyr::filter(., varname == 'hpils') %>%
	# 	dplyr::mutate(
	# 		.,
	# 		seas = 
	# 		{ts(.$value, start = c(year(.$date[1]), month(.$date[1])), freq = 12)} %>%
	# 		seasonal::seas(.) %>%
	# 		predict(.)
	# 		) %>%
	# 	dplyr::select(., -value)
	# 
	# df =
	# 	dplyr::left_join(p$h$sourceDf, seasDf, by = c('date', 'varname', 'freq')) %>%
	# 	dplyr::mutate(., value = ifelse(is.na(seas), value, seas)) %>%
	# 	dplyr::select(., -seas)
	# 
	# p$h$seasDf <<- df
})
```

## Stationarity
```{r}
local({

	resDfs =
		p$variables %>%
		lapply(., function(x) {
			
			message(x$varname)
			
			lapply(c('st', 'd1', 'd2') %>% setNames(., .), function(form) {
			
				if (x[[form]] == 'none') return(NULL)
				# Now iterate through sub-frequencies
				lapply(x$h$base, function(df)
					df %>%
						dplyr::arrange(., date) %>%
						dplyr::mutate(
							.,
							value = {
								if (x[[form]] == 'base') value
								else if (x[[form]] == 'dlog') dlog(value)
								else if (x[[form]] == 'diff1') diff1(value)
								else if (x[[form]] == 'pchg') pchg(value)
								else if (x[[form]] == 'apchg') apchg(value, {if (x$freq == 'q') 4 else 12})
								else stop ('Error')
								}
							) %>%
						na.omit(.)
					)
				}) %>%
				purrr::compact(.)
			})

    for (varname in names(resDfs)) {
		for (form in names(resDfs[[varname]])) {
			p$variables[[varname]]$h[[form]] <<- resDfs[[varname]][[form]]
		}
	}
})
```

# Checks

## Add histEnd to each variable
```{r}
local({
	
	for (varname in names(p$variables)) {
		p[[varname]]$qEnd <<- max(p$variables[[varname]]$h$base$q$date)
		p[[varname]]$mEnd <<- {
			if(p$variables[['gdp']]$freq %in% c('d', 'w', 'm'))
				max(p$variables[[varname]]$h$base$m$date) else NA
			}
	}

})
```


# Aggregate

## Flat
```{r}
local({
	
	flat =
		purrr::imap_dfr(p$variables, function(x, varname)
			purrr::imap_dfr(x$h, function(y, form)
				purrr::imap_dfr(y, function(z, freq)
					z %>% dplyr::mutate(., freq = freq, form = form, varname = varname)
					)
				)
			) %>%
		dplyr::filter(., freq %in% c('m', 'q'))

	h$flat <<- flat
})
```

## Create monthly/quarterly matrixes
```{r}
local({
	
	wide =
		h$flat %>%
		as.data.table(.) %>%
		split(., by = 'form') %>%
		lapply(., function(x)
			split(x, by = 'freq') %>%
				lapply(., function(y)
					as_tibble(y) %>%
						dplyr::select(., -freq, -form) %>%
						tidyr::pivot_wider(., names_from = varname) %>%
						dplyr::arrange(., date)
					)
			)

	h$base <<- wide$base
	h$st <<- wide$st
	h$d1 <<- wide$d1
	h$d2 <<- wide$d2
})
```


# Finalize
```{r}
local({
	
	db = dbConnect(
		RPostgres::Postgres(),
		dbname = CONST$DB_DATABASE,
		host = CONST$DB_SERVER,
		port = 5432,
		user = CONST$DB_USERNAME,
		password = CONST$DB_PASSWORD
		)
	
	# Get last vintage for each tskey/varname
	extVals =
		DBI::dbGetQuery(db, 'SELECT * FROM ext_tsvalues') %>%
		as_tibble(.) %>%
		group_by(., tskey, varname) %>%
		mutate(., vdate_last = max(vdate)) %>%
		filter(., vdate == vdate_last) %>%
		ungroup(.)

})
```

## Export
```{r}
local({
  
    saveRDS(
    	list(p = p, h = h, m = m),
    	str_glue(OUTPUT_DIR, '/[{p$VINTAGE_DATE}] m1.rds')
    	)
	
})
```
